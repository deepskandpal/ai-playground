{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4878b0",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Policy Search and OpenAI Gym\n",
    "\n",
    "This notebook introduces the foundational concepts of policy search in reinforcement learning (RL) and demonstrates practical usage of OpenAI Gym with the classic CartPole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d82988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic policy: always returns the same action for a given observation\n",
    "\n",
    "def deterministic_policy(obs):\n",
    "    # Example: always move right\n",
    "    return 1\n",
    "\n",
    "# Stochastic policy: returns an action sampled from a probability distribution\n",
    "import numpy as np\n",
    "\n",
    "def stochastic_policy(obs):\n",
    "    # Example: 70% chance to move right, 30% left\n",
    "    return np.random.choice([0, 1], p=[0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617dbcf",
   "metadata": {},
   "source": [
    "## 1. Policy Search in Reinforcement Learning\n",
    "\n",
    "A **policy** is the strategy an agent uses to decide what action to take given an observation from the environment. It is the agent's \"brain\" or decision-making function.\n",
    "\n",
    "**Types of Policies:**\n",
    "- **Deterministic:** Always outputs the same action for a given observation.\n",
    "- **Stochastic:** Outputs a probability distribution over actions and samples from it.\n",
    "\n",
    "**Policy Representations:**\n",
    "- Rule-based systems\n",
    "- Lookup tables\n",
    "- Neural networks (common in Deep RL)\n",
    "\n",
    "**Policy Search:**\n",
    "The process of finding the best policy parameters (e.g., probabilities, neural network weights) to maximize cumulative reward.\n",
    "\n",
    "**Methods for Policy Search:**\n",
    "1. **Brute Force:** Try many parameter combinations and pick the best.\n",
    "2. **Genetic Algorithms:** Evolve a population of policies over generations.\n",
    "3. **Policy Gradients:** Use optimization (gradient ascent) to improve policy parameters based on reward gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe2637",
   "metadata": {},
   "source": [
    "## 2. Setting Up and Using OpenAI Gym\n",
    "\n",
    "[OpenAI Gym](https://www.gymlibrary.dev/) is a toolkit that provides a wide variety of simulated environments for RL. It is a standard platform for training and evaluating RL agents.\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install -U gym\n",
    "```\n",
    "\n",
    "Some environments may require extra dependencies for rendering (e.g., `pyglet`, `pygame`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb19f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "# Import gym and check version\n",
    "import gymnasium as gym\n",
    "print('Gym version:', gym.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f26ffb",
   "metadata": {},
   "source": [
    "## 3. Exploring the CartPole Environment\n",
    "\n",
    "The CartPole environment is a classic RL \"hello world.\" The goal is to keep a pole balanced upright on a moving cart for as long as possible.\n",
    "\n",
    "Let's walk through the basic steps to interact with this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b066b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial observation: [-0.02014128  0.01499837  0.00417846  0.02987751]\n",
      "Observation space: Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the CartPole environment\n",
    "env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "obs, info = env.reset()\n",
    "print('Initial observation:', obs)\n",
    "\n",
    "# Inspect the observation space\n",
    "print('Observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e5b156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Render the environment\n",
    "# env.render()  # Uncomment to visualize (may not work on headless servers)\n",
    "\n",
    "# Check the action space\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31aef649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New observation: [-0.01984131  0.21006015  0.00477601 -0.26148415]\n",
      "Reward: 1.0\n",
      "Done: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "# Take a step in the environment\n",
    "action = 1  # Accelerate right\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "print('New observation:', obs)\n",
    "print('Reward:', reward)\n",
    "print('Done:', done)\n",
    "print('Info:', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873d6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()  # Render the environment (if supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a76022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment when done\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ea2e3",
   "metadata": {},
   "source": [
    "## 4. Implementing a Simple Hardcoded Policy\n",
    "\n",
    "Let's implement a basic policy for CartPole: if the pole is leaning left (angle < 0), move left; otherwise, move right. We'll run this policy for multiple episodes and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b6740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean episode reward: 42.51\n",
      "Max episode reward: 68.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset()\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print('Mean episode reward:', np.mean(totals))\n",
    "print('Max episode reward:', np.max(totals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
